{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95392963-17ab-441f-a45b-5145b89a4305",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8d19ea6-03a5-4e6b-9e97-26ffff7c16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Qfunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(state_dim, 64)\n",
    "        self.linear_2 = nn.Linear(64, 64)\n",
    "        self.linear_3 = nn.Linear(64, action_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, states):\n",
    "        hidden = self.linear_1(states)\n",
    "        hidden = self.activation(hidden)\n",
    "        hidden = self.linear_2(hidden)\n",
    "        hidden = self.activation(hidden)\n",
    "        actions = self.linear_3(hidden)\n",
    "        \n",
    "        return actions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a9aee95-c42c-4d77-bb41-85a9b36b823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim, gamma = .99, batch_size=64, lr=.001, eps_decrease = .01, eps_min = .01):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.q_function = Qfunction(state_dim, action_dim)\n",
    "        self.epsilon = 1\n",
    "        self.eps_decrease = eps_decrease\n",
    "        self.eps_min = eps_min\n",
    "        self.memory = []\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = torch.optim.Adam(self.q_function.parameters(), lr = lr)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        q_values = self.q_function(torch.FloatTensor(state))\n",
    "        \n",
    "        argmax_action = torch.argmax(q_values)\n",
    "        probs = self.epsilon * np.ones(self.action_dim) / self.action_dim\n",
    "        probs[argmax_action] += 1 - self.epsilon\n",
    "        \n",
    "        action = np.random.choice(np.arange(self.action_dim), p = probs)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def get_batch(self):        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states, actions, rewards, dones, next_states = [], [], [], [], []\n",
    "        #print(batch)\n",
    "        \n",
    "        for five in batch:\n",
    "            states.append(five[0])\n",
    "            actions.append(five[1])\n",
    "            rewards.append(five[2])\n",
    "            dones.append(five[3])\n",
    "            next_states.append(five[4])\n",
    "            \n",
    "        #states, actions, rewards, dones, next_states = list(zip(*batch)) # вместо цикла на 7 строк\n",
    "            \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        \n",
    "        #states, actions, rewards, dones, next_states = map(torch.FloatTensor, list(zip(*batch))) #вместо всего выше написанного\n",
    "        \n",
    "        return states, actions, rewards, dones, next_states\n",
    "    \n",
    "    def fit(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, done, next_state])\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:  \n",
    "            states, actions, rewards, dones, next_states = self.get_batch()\n",
    "\n",
    "            #q_values = []\n",
    "            #for i in range(batch_size):\n",
    "            #    q_values.append(self.q_function(states[i][actions[i]]))\n",
    "            #q_values = torch.FloatTensor(q_values)\n",
    "\n",
    "            #q_values = self.q_function(states) # матрица строки- эл-т батча, столбцы - д-е\n",
    "            #targets = q_values.clone()\n",
    "            #for i in range(self.batch_size):\n",
    "            #    target[i][actions[i]] = rewards[i] + self.gamma * (1 - dones[i]) * max(self.q_function(next_states[i]))\n",
    "\n",
    "            targets = rewards + self.gamma * (1 - dones) * torch.max(self.q_function(next_states), dim =1).values #????\n",
    "            q_values = self.q_function(states)[torch.arange(self.batch_size), actions] #check\n",
    "\n",
    "            loss = torch.mean((q_values - targets.detach()) ** 2)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            if self.epsilon > self.eps_min:\n",
    "                self.epsilon -= self.eps_decrease\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7105dc8b-a7a8-4c2b-89be-a2ccca876ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0\n",
      "11.0\n",
      "34.0\n",
      "13.0\n",
      "24.0\n",
      "10.0\n",
      "13.0\n",
      "12.0\n",
      "10.0\n",
      "10.0\n",
      "13.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "11.0\n",
      "11.0\n",
      "14.0\n",
      "18.0\n",
      "9.0\n",
      "11.0\n",
      "24.0\n",
      "19.0\n",
      "18.0\n",
      "18.0\n",
      "17.0\n",
      "15.0\n",
      "22.0\n",
      "23.0\n",
      "22.0\n",
      "14.0\n",
      "21.0\n",
      "53.0\n",
      "47.0\n",
      "30.0\n",
      "43.0\n",
      "132.0\n",
      "58.0\n",
      "121.0\n",
      "72.0\n",
      "89.0\n",
      "40.0\n",
      "44.0\n",
      "78.0\n",
      "44.0\n",
      "54.0\n",
      "106.0\n",
      "76.0\n",
      "181.0\n",
      "142.0\n",
      "156.0\n",
      "172.0\n",
      "199.0\n",
      "226.0\n",
      "227.0\n",
      "295.0\n",
      "184.0\n",
      "172.0\n",
      "168.0\n",
      "192.0\n",
      "184.0\n",
      "174.0\n",
      "159.0\n",
      "163.0\n",
      "170.0\n",
      "179.0\n",
      "172.0\n",
      "178.0\n",
      "178.0\n",
      "161.0\n",
      "172.0\n",
      "156.0\n",
      "158.0\n",
      "156.0\n",
      "170.0\n",
      "153.0\n",
      "177.0\n",
      "159.0\n",
      "165.0\n",
      "166.0\n",
      "137.0\n",
      "173.0\n",
      "151.0\n",
      "163.0\n",
      "186.0\n",
      "174.0\n",
      "157.0\n",
      "200.0\n",
      "169.0\n",
      "175.0\n",
      "167.0\n",
      "163.0\n",
      "192.0\n",
      "239.0\n",
      "213.0\n",
      "203.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQN(state_dim, action_dim)\n",
    "\n",
    "episode_n = 100\n",
    "t_max = 500\n",
    "\n",
    "for i in range(episode_n):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        agent.fit(state, action, reward, done, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff15aff-ff17-4fd7-952c-1b44c18fc801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6863dd9-2e84-440d-9ffb-c6983f7eeaab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e235cc-2078-41f6-97dc-e503bf01f43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b620cf-b26f-46b2-a0f2-d79249efaa7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
